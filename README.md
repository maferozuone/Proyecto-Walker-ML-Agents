ğŸ¦µ Walker ML-Agents â€“ Proyecto de LocomociÃ³n BÃ­peda
Aprendizaje por Refuerzo Profundo en Unity + ML-Agents

Este repositorio contiene un proyecto completo para entrenar y evaluar un agente bÃ­pedo (Walker) en Unity utilizando ML-Agents, comparando el rendimiento de los algoritmos PPO y SAC, junto con diferentes configuraciones de recompensas.

El objetivo principal es estudiar cÃ³mo influye el diseÃ±o de recompensas y el algoritmo seleccionado en la estabilidad, velocidad y naturalidad de la locomociÃ³n del agente.

ğŸ“ Estructura del Repositorio
ğŸ“¦ Proyecto
 â”£ Assets/               â†’ Modelos, scripts y configuraciones de Unity
 â”£ Scenes/               â†’ Escena principal del Walker
 â”£ Scripts/
 â”ƒ â”— WalkerAgent.cs      â†’ LÃ³gica RL del agente
 â”£ config/
 â”ƒ â”£ WalkerPPO.yaml      â†’ ConfiguraciÃ³n del algoritmo PPO
 â”ƒ â”— WalkerSAC.yaml      â†’ ConfiguraciÃ³n del algoritmo SAC
 â”£ results/
 â”ƒ â”£ export_csv.py       â†’ Exporta logs de TensorBoard a CSV
 â”ƒ â”£ plot_runs.py        â†’ Genera grÃ¡ficas (avg, max, std, comparaciones)
 â”ƒ â”— (carpetas de runs)  â†’ Resultados de entrenamiento
 â”£ Builds/               â†’ Build del entorno Unity
 â”— README.md

ğŸ› ï¸ Requisitos
Software necesario

Unity 2022.x (compatible con ML-Agents 2.x)

Python 3.9 (recomendado)

ML-Agents Toolkit (mlagents y mlagents-envs)

PyTorch

TensorBoard

(Opcional) Matplotlib, Pandas para anÃ¡lisis

âš™ï¸ InstalaciÃ³n del entorno ML-Agents
python -m venv mlagents-env
mlagents-env\Scripts\activate

pip install mlagents==0.30.0
pip install torch torchvision torchaudio
pip install tensorboard
pip install matplotlib pandas

ğŸ® Uso del Proyecto en Unity

Abrir Unity Hub

Seleccionar Open Project

Elegir la carpeta principal del repositorio

Abrir la escena:

Assets/Scenes/WalkerScene.unity


Dentro encontrarÃ¡s:

El Walker

El objetivo (Target)

Sensores

Articulaciones

Comportamientos ML-Agents

ğŸ¤– Entrenamiento del Agente
Entrenar con PPO
mlagents-learn config/WalkerPPO.yaml --run-id=WalkerPPO --env="Builds/Walker.exe" --no-graphics

Entrenar con SAC
mlagents-learn config/WalkerSAC.yaml --run-id=WalkerSAC --env="Builds/Walker.exe" --no-graphics


Los resultados aparecerÃ¡n en:

results/WalkerPPO/
results/WalkerSAC/

ğŸ“Š VisualizaciÃ³n del Entrenamiento (TensorBoard)
tensorboard --logdir results


Abrir en navegador:

http://localhost:6006

ğŸ“¤ Exportar resultados a CSV

Desde la carpeta results/:

python export_csv.py


El script:

lee cada carpeta con logs

extrae las recompensas

genera un CSV por run

ğŸ“ˆ Generar grÃ¡ficas comparativas
python plot_runs.py


Este script genera automÃ¡ticamente:

Recompensa promedio (moving average)

Recompensa mÃ¡xima (max per window)

DesviaciÃ³n estÃ¡ndar

ComparaciÃ³n entre runs seleccionadas

GrÃ¡ficas individuales por cada run

Y guarda las imÃ¡genes en:

results/*.png

ğŸ¯ Uso del modelo entrenado

Una vez entrenado, Unity generarÃ¡ un archivo .nn.
Puedes cargarlo en:

Behavior Parameters â†’ Model


AsÃ­ puedes correr el agente sin entrenamiento.

ğŸ“Œ Objetivos del proyecto

Comparar PPO vs SAC en locomociÃ³n bÃ­peda

Estudiar cÃ³mo el diseÃ±o de recompensas afecta el aprendizaje

Obtener movimientos mÃ¡s naturales y estables

Analizar mÃ©tricas cuantitativas mediante grÃ¡ficas

Evaluar desempeÃ±o bajo distintas condiciones

ğŸ‘¨â€ğŸ’» Autor

Manuel Fernando Rocha Zuleta, Miguel Enrique Galindo Florez
Universidad Nacional de Colombia â€“ Sede La Paz
Programa de IngenierÃ­a MecatrÃ³nica
